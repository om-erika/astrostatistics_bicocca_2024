{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probability & Statistics: I\n",
    "\n",
    "*Davide Gerosa (Milano-Bicocca)*\n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Ivezic textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 3. \n",
    "- [David Hogg: \"Data analysis recipes: Probability calculus for inference\"](https://arxiv.org/abs/1205.4446)\n",
    "\n",
    "\n",
    "This course is based on previous work by many people. See [here]((https://github.com/dgerosa/astrostatistics_bicocca_2024/blob/main/README.md) for credits.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries and notation <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "* **\"Statistics\"** = extracting knowledge from data.\n",
    "* **\"Astrostatistics\"** = extracting knowledge from astronomical data.\n",
    "* **\"Knowledge\"** = summary (physical or phenomenological) of data behavior.\n",
    "* **\"Data\"** = result of measurements.\n",
    "\n",
    "In the textbook, $x$ is a scalar quantity that is measured $N$ times to form a dataset.\n",
    "\n",
    "* $x_i$ is a single measurement with $i=1,...,N$.\n",
    "* $\\{x_i\\}$ refers to the set of all N measurements comprising the dataset. \n",
    "\n",
    "Our data can be real numbers, discrete labels (strings or numbers), or even \"missing values\" (we sometimes pad our datasets with NaNs in this case). \n",
    "$\\color{red}{\\text{eg: you miss one night of measurement}}$.\n",
    "\n",
    "**Goal of data mining & statistical inference:**\n",
    "> We are generally trying to *estimate* $h(x)$, the *true* generating distribution from which $\\{x_i\\}$ are drawn. \n",
    "\n",
    "* $h(x)$ is the **probability density (distribution) function** or the **\"pdf\"** and $h(x)dx$ is the propobability of a value lying between $x$ and $x+dx$. This distribution can have several levels-- the population distribution of events (e.g. source redshifts), and a measurement uncertainity distribution that blurs our measured data away from true values.\n",
    "\n",
    "* The \"left to right\" integral of $h(x)$ is the **cumulative distribution function** (**\"cdf\"**), $H(x) = \\int_{-\\infty}^x h(x')dx'$. The inverse function of the cdf is the **quantile function**, e.g. what $x$ value has 90% of the distribution below it?\n",
    "\n",
    "* While $h(x)$ is the \"true\" pdf (or **population** pdf), what we *measure* from the data is the **empirical** pdf, which is denoted $f(x)$.  So, $f(x)$ is a *model* of $h(x)$.  In principle, with infinite data $f(x) \\rightarrow h(x)$, but in reality the blurring effect of measurement errors keep this from being strictly true. Likewise, the empirical cdf is denoted $F(x)$.\n",
    "\n",
    "* If we are attempting to guess a physical *model* for $h(x)$, then the process is ***parametric***.  With a model solution we can generate new data that should mimic what we measure.  If we are not attempting to guess a model, then the process is ***non-parametric***, i.e. we are just trying to describe the data behavior in a compact practical way. $\\color{red}{\\text{non parametric: you don't force a theoretical model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picturing (i.e. understanding!) a pdf is not a trivial task!\n",
    "\n",
    "Let's see an example. This cell will start with a generating distribution $h(x)$, draw a number of random samples as data $\\{x_i\\}$, and then fit these data with a parametric and non-parametric model $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\theano\\scalar\\basic.py:2412: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  self.ctor = getattr(np, o_type.dtype)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# This function adjusts matplotlib settings for a uniform feel in the textbook.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Note that with usetex=True, fonts are rendered with LaTeX.  This may\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# result in an error if LaTeX is not installed on your system.  In that case,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# you can set usetex to False.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup_text_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mastroML\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_text_plots\n\u001b[0;32m     27\u001b[0m setup_text_plots(fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, usetex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInlineBackend.figure_format=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretina\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m # very useful command for high-res images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\astroML\\plotting\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiaxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAxes\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_text_plots\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_regressions, plot_regression_from_trace\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\astroML\\plotting\\regression.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mastroML\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TLS_logL, LinearRegression\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TLS:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_m_b\u001b[39m(beta):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\astroML\\linear_model\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, PolynomialRegression, BasisFunctionRegression\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_regression_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegressionwithErrors\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NadarayaWatson\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTLS\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TLS_logL\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\astroML\\linear_model\\linear_regression_errors.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pymc3\\__init__.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemver\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\n\u001b[0;32m     25\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpymc3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mhandlers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\theano\\__init__.py:83\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# This is the api version for ops that generate C code.  External ops\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# might need manual changes if this number goes up.  An undefined\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# __api_version__ can be understood to mean api version 0.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This number is not tied to the release version and should change\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# very rarely.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m __api_version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scalar, tensor\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     85\u001b[0m     In,\n\u001b[0;32m     86\u001b[0m     Mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m     shared,\n\u001b[0;32m     94\u001b[0m )\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function, function_dump\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\theano\\scalar\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic_scipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\theano\\scalar\\basic.py:2460\u001b[0m\n\u001b[0;32m   2456\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2457\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m-> 2460\u001b[0m convert_to_bool \u001b[38;5;241m=\u001b[39m \u001b[43mCast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvert_to_bool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2461\u001b[0m convert_to_int8 \u001b[38;5;241m=\u001b[39m Cast(int8, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_int8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2462\u001b[0m convert_to_int16 \u001b[38;5;241m=\u001b[39m Cast(int16, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_int16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\theano\\scalar\\basic.py:2412\u001b[0m, in \u001b[0;36mCast.__init__\u001b[1;34m(self, o_type, name)\u001b[0m\n\u001b[0;32m   2410\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(specific_out(o_type), name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   2411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_type \u001b[38;5;241m=\u001b[39m o_type\n\u001b[1;32m-> 2412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py:319\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    314\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Based on Ivezic v2, Figure 6.8; edited by G. T. Richards, S. R. Taylor, and D. Gerosa\n",
    "\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from astropy.visualization import hist\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "%config InlineBackend.figure_format='retina' # very useful command for high-res images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "# In reality nature generates data for you\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 2000 # number of data points\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "hx = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N), random_state=random_state)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the numbers you have. \n",
    "#Say these are stellar flux measurements, or a time series from a gravitational-wave detector\n",
    "\n",
    "print(x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do the easiest thing ever.\n",
    "plt.hist(x,bins=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$\\color{red}{\\text{Notice that changing the number of bins change the number of peaks; what can i say about the distribution?}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "xgrid = np.linspace(-10, 30, 1000)\n",
    "\n",
    "# True distribution: you typically don't have it! Only have the samples!\n",
    "if False:\n",
    "    ax.plot(xgrid, hx(xgrid), ':', color='black', zorder=3,\n",
    "            label=\"$h(x)$, Generating Distribution\")\n",
    "\n",
    "# A simple histogram\n",
    "# But try changing the number of bins!\n",
    "if True:\n",
    "    ax.hist(x,density=True,color='C0',bins=100,histtype='step',lw=2, label='Histogram, 100 bins')\n",
    "\n",
    "# Something more sophisticated: Kernel Density Estimation\n",
    "# But try changing the bandwith! \n",
    "if True:\n",
    "    kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "    kde.fit(x[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(xgrid[:, None]))\n",
    "    ax.plot(xgrid, dens_kde, '-', color='C1', zorder=3,\n",
    "            label=\"$f(x)$, non-parametric (KDE)\")\n",
    "\n",
    "# Use Gaussian Mixtures with a pre-defined number of clusters (13)\n",
    "if True:\n",
    "    gmm = GaussianMixture(n_components=13).fit(x.reshape(-1, 1))\n",
    "    logprob = gmm.score_samples(xgrid.reshape(-1, 1))\n",
    "    fx = lambda j : np.exp(gmm.score_samples(j.reshape(-1, 1)))\n",
    "    ax.plot(xgrid, fx(np.array(xgrid)), '-', color='C2',\n",
    "            label=\"$f(x)$, parametric (13 Gaussians)\")\n",
    "\n",
    "# Plot cosmetics\n",
    "ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel('$p(x)$',fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_xlabel('$x$',fontsize=14)\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34357186498273545\n",
      "[0.15440079]\n",
      "[0.23034853]\n"
     ]
    }
   ],
   "source": [
    "# For instance in a single point:\n",
    "print(hx(9))             # h(x), the true distribution\n",
    "\n",
    "print(fx(np.array([9]))) # f(x) for a parametric model\n",
    "print(np.exp(kde.score_samples(np.atleast_2d(9)))) # f(x) for non-parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3108859528.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    $\\color{red}{\\text{Both cannot predict it well}}$\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "$\\color{red}{\\text{Both cannot predict it well}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on uncertainties and errors\n",
    "\n",
    "* Technically, errors are systematic biases that we can not mitigate through collecting lots and lots of data. \n",
    "* Statistical uncertainties are the result of random measurement uncertainty. \n",
    "* But \"error\" will be used for both, and denoted as either statistical errors (error bars) or systematic errors (biases).\n",
    "\n",
    "\n",
    "* Statistical error distributions (error bars) that vary from data point to data point are called **heteroscedastic errors**. If they are the same for all points then they are **homoscedastic errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  We could summarize the goal of this course as an attempt to \n",
    "\n",
    "1) estimate $f(x)$ from some real (possibly multi-dimensional) data set, \n",
    "\n",
    "2) find a way to describe $f(x)$ and its uncertainty, \n",
    "\n",
    "3) compare it to models of $h(x)$, and then \n",
    "\n",
    "4) use the knowledge that we have gained to interpret/predict new measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "> $p(A)$ = the probability of $A$ (or the probability density at $A$; I'm sloppy!) \n",
    "\n",
    "e.g. the probability that an observed object is a galaxy. This does not mean that the object is in some sort of Schrodinger's cat quantum uncertainity...*the probability reflects our current state of knowledge of the object, and our belief that it is a galaxy*. \n",
    "$\\color{red}{\\text{That is related to both the datas and what you know already (eg. context: the fact that a student at the bottom of a class has a laptop in front of them is related to a likelihood and it's a prior)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probability axioms (Kolmogorov)\n",
    "\n",
    "1. $p(A)\\geq0 \\quad\\forall\\, A$\n",
    "2. $p(\\Omega) = 1$, where $\\Omega$ is the set of all outcomes, i.e. the sum/integral of all possible outcomes is unity\n",
    "3. $p(\\cup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty p(A_i)$ if all events are independent\n",
    "\n",
    "$A \\cup B$ is the *union* of sets $A$ and $B$. **Read as A OR B.**\n",
    "\n",
    "$A \\cap B$ is the *intersection* of sets $A$ and $B$. **Read as A AND B.** Different notations $p(A \\cap B) = p(AB) = p(A,B) = p(A\\,\\mathrm{and}\\,B)$. We will use the comma notation throughout. \n",
    "\n",
    "If we have two events, $A$ and $B$, the possible combinations are illustrated by the following figure:\n",
    "![Figure 3.1](http://www.astroml.org/_images/fig_prob_sum_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The probability that ***either*** $A$ ***or*** $B$ will happen (which could include both) is the *union*, given by\n",
    "$$p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$\n",
    "The figure makes it clear why the last term is necessary.  Since $A$ and $B$ overlap, we are double-counting the region where *both* $A$ and $B$ happen, so we have to subtract this out.  \n",
    "\n",
    "\n",
    "* The probability that ***both*** $A$ ***and*** $B$ will happen, $p(A \\cap B)$, is \n",
    "$$p(A \\cap B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "where p(A|B) is the probability of A *given that* B is true and is called the **conditional probability**.  So the $|$ is short for \"given\".\n",
    "\n",
    "\n",
    "* The **law of total probability** says that (for independent $B_i$)\n",
    "$$p(A) = \\sum_ip(A|B_i)p(B_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is important to realize that the following is *always* true $\\color{red}{\\text{I take all cases of B, and consider the cases in which also A happens}}$:\n",
    "\n",
    "$$p(A,B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n",
    "However, if $A$ and $B$ are independent, then $p(A|B)=p(A)$ and $p(B|A)=p(B)$ and\n",
    "\n",
    "$$p(A,B) = p(A)p(B)$$\n",
    "\n",
    "**EXAMPLE** (classic marbles in bag scenario)\n",
    "\n",
    "If you have a bag with 5 marbles (3 yellow and 2 blue) and you want to know the probability of picking 2 yellow marbles in a row, that would be\n",
    "\n",
    "$$p(Y_1,Y_2) = p(Y_1)p(Y_2|Y_1).$$\n",
    "\n",
    "$p(Y_1) = \\frac{3}{5}$ since you have an equally likely chance of drawing any of the 5 marbles.\n",
    "\n",
    "If you did not put the first marble back in the back after drawing it (sampling *without* \"replacement\"), then the probability\n",
    "\n",
    "$p(Y_2|Y_1) = \\frac{2}{4}$, so that\n",
    "\n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{2}{4} = \\frac{3}{10}.$$\n",
    "\n",
    "But if you put the first marble back, then\n",
    "\n",
    "$p(Y_2|Y_1) = \\frac{3}{5} = p(Y_2)$, so that \n",
    "\n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{3}{5} = \\frac{9}{25}.$$\n",
    "\n",
    "In the first case $A$ and $B$ (or rather $Y_1$ and $Y_2$) are *not* independent, whereas in the second case they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Need more help with this?**  Try watching some Khan Academy videos and working through the exercises:\n",
    "* [https://www.khanacademy.org/math/probability/probability-geometry](https://www.khanacademy.org/math/probability/probability-geometry)\n",
    "* [https://www.khanacademy.org/math/precalculus/prob-comb](https://www.khanacademy.org/math/precalculus/prob-comb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "In the following figure, we have a 2-D distribution in $x-y$ parameter space.  Here $x$ and $y$ are ***not*** independent as, once you pick a $y$, your values of $x$ are constrained. $\\color{red}{\\text{The first is not conditional because is the probability of x (y) integrated on all y (x). The second is conditional because I force y to be in that small range}}$\n",
    "\n",
    "![http://www.astroml.org/_images/fig_conditional_probability_1.png](http://www.astroml.org/_images/fig_conditional_probability_1.png)\n",
    "\n",
    "We have that \n",
    "$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "\n",
    "We can define the ***marginal probability*** as\n",
    "$$p(x) = \\int p(x,y)dy,$$\n",
    "\n",
    "where marginal means projecting on to one axis (integrating over the unwanted variable). The **marginal** distributions are shown on the left and bottom sides of the left panel.  As the equation above says, this is just the integral along the $x$ direction for a given $y$ (left side panel) or the integral along the $y$ direction for a given $x$ (bottom panel).  \n",
    "\n",
    "The three panels on the right show the ***conditional probability*** (of $x$) for three $y$ values: $$p(x|y=y_0)$$  These are just normalized \"slices\" through the 2-D distribution.\n",
    "\n",
    "The marginal probability of $x$ can be re-written as\n",
    "\n",
    "$$p(x) = \\int p(x|y)p(y) dy$$\n",
    "\n",
    "But since $p(x|y)p(y) = p(y|x)p(x)$, we can write\n",
    "\n",
    "> $$p(y|x) = \\frac{p(x|y)p(y)}{p(x)} = \\frac{p(x|y)p(y)}{\\int p(x|y)p(y) dy}$$\n",
    "\n",
    "which in words says that\n",
    "\n",
    "> the (conditional) probability of $y$ given $x$ is just the (conditional) probability of $x$ given $y$ times the (marginal) probability of $y$ divided by the (marginal) probability of $x$, where the latter is just the integral of the numerator.\n",
    "\n",
    "This is **Bayes' Theorem**, which itself is not at all controversial, though its application can be as we'll discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem Example: Lego's <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "An example with Lego's (it's awesome, let's have a look!):\n",
    "[https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)\n",
    "\n",
    "(This example comes from a very nice book: [Bayesian Statistics the Fun Way](https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem Example: Contingency Table  and COVID Tests\n",
    "\n",
    "We can also use Bayes' rule to learn something about false positives and false negatives.\n",
    "\n",
    "Let's look at COVID tests.  The test can be positive ($T=1$) or negative ($T=0$) and one can either have the disease ($D=1$) or not ($D=0$).  So, there are 4 possible combinations:\n",
    "$$T=0; D=0 \\;\\;\\;  {\\rm true \\; negative}$$\n",
    "$$T=0; D=1 \\;\\;\\; {\\rm false \\; negative}$$\n",
    "$$T=1; D=0 \\;\\;\\; {\\rm false \\; positive}$$\n",
    "$$T=1; D=1 \\;\\;\\; {\\rm true \\; positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All else being equal, you have a 50% chance of being misdiagnosed.  Not good!  But the probability of disease and the accuracy of the test presumably are not random.\n",
    "\n",
    "If the rates of false positive and false negative are:\n",
    "$$p(T=1|D=0) = \\epsilon_{\\rm FP}$$\n",
    "$$p(T=0|D=1) = \\epsilon_{\\rm FN}$$\n",
    "\n",
    "then the true positive and true negative rates are just:\n",
    "$$p(T=0| D=0) = 1-\\epsilon_{\\rm FP}$$\n",
    "$$p(T=1| D=1) = 1-\\epsilon_{\\rm FN}$$\n",
    "\n",
    "Let's assume that $\\epsilon_{\\rm FP}=0.02$ and $\\epsilon_{\\rm FN}=0.001$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In graphical form this $p(T|D)$ matrix is:\n",
    "![http://www.astroml.org/_images/fig_contingency_table_1.png](http://www.astroml.org/_images/fig_contingency_table_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\color{red}{\\text{We have p(T|D) because the test is tested on sick patients, but we care about p(D|T)}}$\n",
    "\n",
    "If we have a **prior** regarding how likely the disease is, we can take this into account.\n",
    "\n",
    "$$p(D=1)=\\epsilon_D$$\n",
    "\n",
    "and then $p(D=0)=1-\\epsilon_D$. Say, $\\epsilon_D$ = 0.01. \n",
    "\n",
    "Now assume that a person tested positive. What is the probability that this person has the disease? Is it 98% \n",
    "because $\\epsilon_{\\rm FP}=0.02$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can't just read $p(D=1|T=1)$ off the table because the table entry is the conditional probability of the *test* given the *disease*, $p(T=1|D=1)$. What we want is the conditional probability of the *disease* given the *test*, that is, $p(D=1|T=1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes' Theorem then can be used to help us determine how likely it is that you have the disease if you tested positive:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{p(T=1|D=1)p(D=1)}{p(T=1)},$$\n",
    "\n",
    "where $$p(T=1) = p(T=1|D=0)p(D=0) + p(T=1|D=1)p(D=1).$$\n",
    "\n",
    "So\n",
    "$$p(D=1|T=1) = \\frac{(1 - \\epsilon_{FN})\\epsilon_D}{\\epsilon_{FP}(1-\\epsilon_D) + (1-\\epsilon_{FN})\\epsilon_D} \\approx \\frac{\\epsilon_D}{\\epsilon_{FP}+\\epsilon_D}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where in the final approximation we assume that all $\\epsilon$ values are small. That means that to get a reliable diagnosis, we need $\\epsilon_{FP}$ to be quite small.  (Because you *want* the probability to be close to unity if you test positive, otherwise it is a *false* positive).\n",
    "\n",
    "In our example, we have a disease rate of 1% ($\\epsilon_D = 0.01$) and a false positive rate of 2% ($\\epsilon_{\\rm FP}=0.02$).  \n",
    "\n",
    "So we have\n",
    "$$p(D=1|T=1) = \\frac{0.01}{0.02+0.01} = 0.333$$\n",
    "\n",
    "Then in a sample of, e.g.,  1000 people, 10 people will *actually* have the disease $(1000*0.01)$, but another 20 $(1000*0.02)$ will test positive! \n",
    "\n",
    "Therefore, in that sample of 30 people who tested positive, only 1/3 has the disease\n",
    "(not 98%!). \n",
    "\n",
    "$\\color{red}{\\text{This is important because most of the time we have p(A|B) and want instead p(B|A)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations of random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is a random variable then $f(x)$ is also a random variable for any function $f$.\n",
    "\n",
    "To transform probability distributions when taking functions of random variables, we can simply use conservation of dimensionless probability, i.e. \n",
    "\n",
    "$$\\mathrm{Prob}(x, x+dx) = \\mathrm{Prob}(y, y+dy)$$\n",
    "\n",
    "$$p(x)dx = p(y)dy.$$ \n",
    "\n",
    "where $y = f(x)$.\n",
    "\n",
    "Thus, $$p(y) = \\left|\\frac{dx}{dy}\\right| p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE**\n",
    "\n",
    "* Let $x$ be a random variable drawn from a uniform distribution between $0$ and $1$. So $p(x) = 1/(1-0) = 1$.  \n",
    "* Let's transform to $y = e^x$.\n",
    "* So $p(y) = \\left|dy/dx\\right|^{-1}p(x) = 1/y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://www.astroml.org/_images/fig_transform_distribution_1.png](https://www.astroml.org/_images/fig_transform_distribution_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem Example: Monty Hall Problem (or \"Deal Or No Deal\") <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "A very famous stats mind trick... You might have seen this already.\n",
    "\n",
    "The [Monty Hall Problem](https://en.wikipedia.org/wiki/Monty_Hall_problem) was originally posed (and solved) in a letter by Steve Selvin to the American Statistician in [1975](https://www.tandfonline.com/doi/abs/10.1080/00031305.1975.10479121). It became famous as a question from reader Craig F. Whitaker's letter quoted in Marilyn vos Savant's \"Ask Marilyn\" column in Parade magazine in 1990\n",
    "\n",
    "\n",
    "You are playing a TV game show and are shown 2 doors.  One has a car behind it, the other a goat.  What are your chances of picking the door with the car?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, now there are 3 doors: one with a car, two with goats.  The game show host asks you to pick a door, but not to open it yet.  Then the host opens one of the other two doors (that you did not pick) and that has a goat.  The host offers you the opportunity to switch doors.\n",
    "\n",
    "- One player decides to switch\n",
    "- Another player prefers to stay with the previous choice\n",
    " \n",
    "![https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/180px-Monty_open_door.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/180px-Monty_open_door.svg.png)\n",
    "\n",
    "- Now a third player comes in who has not seen what happened earlier. They pick a door.\n",
    "\n",
    "*Which of the three players is most likely to win?*\n",
    "\n",
    "You might know or remember the answer already... but don't think now! **Let's simulate it**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to get you hands dirty! Open a jupyter notebook and code it up! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the Monty Hall Problem\n",
    "\n",
    "- Simulate three doors, one car, and two goats.\n",
    " - Simulate three players: the switcher, the conservative, and the newcomer. \n",
    " - Record who wins.\n",
    " - Repeat it many times.\n",
    " - Which player do you want to be?\n",
    " \n",
    " - What would happen if you had 100 doors to choose from and the presenter opens 98 or them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My implementation](https://github.com/dgerosa/astrostatistics_bicocca_2024/blob/main/solutions/S01_monty_hall.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf of the log\n",
    "1. Use `numpy` to draw $N$ (where $N$ is large...) random samples from a uniform distribution between $0.1$ and $10$, and store these samples as $x$.\n",
    "2. Use matplotlib to make a histogram of these samples.\n",
    "3. Compute the base-10 log of your array $x$, and store this as $y$.\n",
    "4. Make another histogram for $y$. Using the equation to transform probability distributions, write what the theoretical pdf of $y$ is, and overplot it onto your histogram.\n",
    "5. Compute the log of the mean of $x$ and the mean of $y$. Now compute the log of the median of $x$ and the median of $y$. \n",
    "\n",
    "You should note that the means are different, but the medians (as it is a cumulative statistic) are the same. The mean is affected by the scale of the sample values, but the median only depends on the ordering of the samples. Monotonic transformations (like taking the log) do not change the ordering of samples.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My implementation](https://github.com/dgerosa/astrostatistics_bicocca_2024/blob/main/lectures/S02_pdfofthelog.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
